# ltrack Monitor Policy

# File Monitoring Configuration
[file_monitor]
enabled = true
directories = [
    "/home/lighthouse/test-ltrack",
]

# Process Execution Monitoring Configuration
[exec_monitor]
enabled = true
watch_commands = [
    "bash",
    "python",
    "nginx"
]

# Network Monitoring Configuration
[network_monitor]
enabled = true
ports = [2333, 8888, 9999, 10086]
protocols = ["tcp", "udp"]

# HTTP Server Configuration
[http_server]
enabled = true
port = 8088
host = "0.0.0.0"

# Logging configuration
[log]
level = "info"
format = "json"
output_path = "/var/log/ltrack/app.log"
max_size = 100    # MB
max_age = 7       # days
max_backups = 5   # files
compress = true

# Storage configuration
# Storage system configuration, used to define how to store collected event data
[storage]
# Whether to enable the storage system
enabled = true
# Storage type: "file" (file), "stdout" (standard output), "socket" (network socket), "syslog" (system log), "kafka" (Kafka)
# Note: To use Kafka output, set type = "kafka" and adapter = "kafka", and enable kafka.enabled = true
type = "kafka"
# Storage format: "json" (JSON format), "text" (text format), "ndjson" (newline-delimited JSON)
format = "json"
# Log adapter: "standard", "elasticsearch", "logstash", "fluentd", "graylog", "kafka"
# Note: When using Kafka, set adapter = "kafka"
adapter = "kafka"
# File path (used when type="file")
file_path = "/var/log/ltrack/events.log"
# Log file rotation settings
max_size = 100    # Maximum size of a single file (MB)
max_age = 7       # Maximum retention days for logs
max_backups = 5   # Maximum number of retained files
compress = true   # Whether to compress old files
# Remote configuration (used when type="socket")
remote_addr = ""  # Remote address
remote_port = 0   # Remote port
# Auto-detect hostname for extra fields (default: true)
# When enabled, the system will automatically detect the current machine's hostname
# and use it as the "host" field in extra_fields if not explicitly configured or set to "localhost"
auto_detect_host = true

# Extra fields specific to the adapter
# These fields will be added to each log record
# Note: If auto_detect_host is enabled (default), the "host" field will be automatically
# set to the current machine's hostname unless explicitly configured to a different value
[storage.extra_fields]
host = "localhost"  # Will be auto-detected if auto_detect_host=true and this is "localhost" or empty
service = "ltrack"
environment = "production"
version = "1.0.0"

# Adapter-specific configuration
# The adapter will format and process events according to these configuration items

# For logstash adapter, define tags
tags = "security,monitoring"

# For fluentd adapter, define tag
tag = "ltrack.events"

# For graylog adapter, define facility name
facility = "ltrack"

# For elasticsearch adapter, define index prefix
index_prefix = "ltrack"

# Kafka Configuration
# To use Kafka as output destination:
# 1. Set storage.type = "kafka" and storage.adapter = "kafka"
# 2. Set kafka.enabled = true
# 3. Configure brokers, topic, and other Kafka settings below
[kafka]
# Whether to enable Kafka producer
enabled = true
# Kafka broker address list (supports multiple brokers for high availability)
brokers = ["0.0.0.0:9092"]
# Target topic
topic = "ltrack-test"
# Client ID
client_id = "ltrack-producer"
# Compression algorithm: none, gzip, snappy, lz4, zstd
compression = "gzip"
# Batch size (number of messages)
batch_size = 100
# Batch byte size
batch_bytes = 1048576  # 1MB
# Batch timeout (milliseconds)
batch_timeout = 1000   # 1 second
# Write timeout (seconds)
write_timeout = 10
# Read timeout (seconds)
read_timeout = 10
# Number of retries
retries = 3

# SASL authentication configuration (optional)
[kafka.sasl]
enabled = false
# Authentication mechanism: PLAIN, SCRAM-SHA-256, SCRAM-SHA-512
mechanism = "PLAIN"
username = ""
password = ""

# TLS configuration (optional)
[kafka.tls]
enabled = false
# Whether to skip certificate verification (for testing only)
insecure_skip_verify = false
# Client certificate file path
cert_file = ""
# Client private key file path
key_file = ""
# CA certificate file path
ca_file = ""